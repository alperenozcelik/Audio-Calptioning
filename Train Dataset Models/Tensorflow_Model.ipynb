{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_Model",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZV7JnqNzkZT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fZ8USFizxk7",
        "outputId": "62eb487e-88b7-489a-f2d4-e985620f8c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(folder_path): \n",
        "\n",
        "  images = []\n",
        "  names = []\n",
        "  max_length = 0\n",
        "  for file in os.listdir(folder_path):\n",
        "    \n",
        "    image_path = os.path.join(folder_path, file)\n",
        "    size = Image.open(image_path).size\n",
        "    image = np.array(Image.open(image_path).resize((128,128)))\n",
        "    image = image.astype('float32')\n",
        "    image = np.delete(image, [3], axis=2)\n",
        "    #Normalization\n",
        "    image /= 255.0\n",
        "    images.append(image)\n",
        "    names.append(file[:-4])\n",
        "    if image.shape[1]>max_length:\n",
        "      max_length = image.shape[1]\n",
        "\n",
        "  for i in range(len(images)):\n",
        "    images[i] = librosa.util.fix_length(images[i], max_length, axis=1)\n",
        "\n",
        "  images = np.array(images)\n",
        "\n",
        "  df = pd.read_csv('/content/drive/MyDrive/Deneme/clotho_metadata_development.csv')\n",
        "\n",
        "  keywords = []\n",
        "  for i in names:\n",
        "    keywords.append(df.keywords[df.file_name == i].to_list()[0].lower().split(';'))\n",
        "\n",
        "  return names,images,keywords"
      ],
      "metadata": {
        "id": "BELLEDthzzRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names, images, keywords = create_dataset(\"/content/drive/MyDrive/Dataset Images\")"
      ],
      "metadata": {
        "id": "9t447uGpz6id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "for i in keywords:\n",
        "  for j in i:\n",
        "    if len(j)>2:\n",
        "      all_words.append(j)\n",
        "\n",
        "len(all_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr6D5Qffz_YH",
        "outputId": "12ad0e78-81f5-4e22-bd5d-7f9286d10605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14371"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "frequency_list = Counter(all_words).most_common(300)"
      ],
      "metadata": {
        "id": "ogO42dL8z__j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords_updated = []\n",
        "for j in range(len(keywords)):\n",
        "  keyword_list = []\n",
        "  for i in frequency_list:\n",
        "    if i[0] in keywords[j]:\n",
        "      keyword_list.append(i[0])\n",
        "  keywords_updated.append(keyword_list)"
      ],
      "metadata": {
        "id": "JZNh6dq40DEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "multihot = mlb.fit_transform(keywords_updated)\n",
        "class_labels = list(mlb.classes_)"
      ],
      "metadata": {
        "id": "OUzW7iNv0E4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, multihot, test_size=0.20, random_state=33)\n",
        "print(X_train.shape,X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw-tRfZD0GwE",
        "outputId": "737970a8-3c0e-40c9-949b-4b99a33d6136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1561, 128, 128, 3) (391, 128, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
      ],
      "metadata": {
        "id": "8ZCKmZ5U02Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reference : https://www.tensorflow.org/tutorials/audio/simple_audio\n",
        "\n",
        "model = models.Sequential([\n",
        "                           \n",
        "    layers.Input(shape=(128,128, 3)),\n",
        "    # Downsample the input.\n",
        "    layers.Resizing(32, 32),\n",
        "    # Normalize.\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(300),\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(),\n",
        "  loss = 'categorical_crossentropy',\n",
        "  metrics = ['accuracy']\n",
        ")\n",
        " \n",
        "history = model.fit(X_train, y_train, epochs=20, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To5FrRAV0I6N",
        "outputId": "0db01456-1b33-40c3-fb76-1ed0f150c74a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 10s 191ms/step - loss: 40.9855 - accuracy: 0.0096\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 40.1662 - accuracy: 0.0167\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 39.7312 - accuracy: 0.0256\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 37.7255 - accuracy: 0.0359\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 37.1629 - accuracy: 0.0327\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 38.3861 - accuracy: 0.0282\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 37.1822 - accuracy: 0.0320\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 38.8218 - accuracy: 0.0237\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 37.7718 - accuracy: 0.0314\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 5s 110ms/step - loss: 36.9956 - accuracy: 0.0269\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 5s 109ms/step - loss: 36.5360 - accuracy: 0.0250\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 35.9892 - accuracy: 0.0269\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 36.4110 - accuracy: 0.0218\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 36.8208 - accuracy: 0.0173\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 5s 108ms/step - loss: 36.3096 - accuracy: 0.0231\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 36.6080 - accuracy: 0.0263\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 36.0195 - accuracy: 0.0218\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 35.8937 - accuracy: 0.0237\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 36.0755 - accuracy: 0.0263\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 5s 109ms/step - loss: 35.9442 - accuracy: 0.0307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PGCL-e6D9TM0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}